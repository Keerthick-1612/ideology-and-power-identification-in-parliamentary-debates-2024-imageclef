{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Specify dataset path and data range [start_idx:end_idx]\n",
    "dataset_path = 'power.csv'\n",
    "data_start_idx = 0\n",
    "data_end_idx = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/path/to/your/power_train.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_save_model(dataset_path, start_idx, end_idx, device):\n",
    "    # Load your dataset\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Select the specified range of data\n",
    "    selected_data = df.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Initialize BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenize input texts and prepare input tensors\n",
    "    tokenized_texts = selected_data['text_en'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=256))\n",
    "    max_len = max(len(tokens) for tokens in tokenized_texts)\n",
    "    padded_sequences = [tokens + [0]*(max_len - len(tokens)) for tokens in tokenized_texts]\n",
    "    input_ids = torch.tensor(padded_sequences).to(device)  # Move input to device\n",
    "    labels = torch.tensor(selected_data['label'].values).to(device)  # Move labels to device\n",
    "\n",
    "    # Create TensorDataset and DataLoader for training\n",
    "    dataset = TensorDataset(input_ids, labels)\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Load pre-trained BERT model for sequence classification\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)  # Move model to device\n",
    "\n",
    "\n",
    "    if os.path.exists(file_path) and file_path.lower().endswith('.pth'):\n",
    "\n",
    "        # Load pre-trained model state if available\n",
    "        ###############################################################################\n",
    "        model.load_state_dict(torch.load('power_train.pth', map_location=device))\n",
    "        ###############################################################################\n",
    "\n",
    "    # Define optimizer and learning rate\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 16\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            batch_input_ids, batch_labels = batch\n",
    "            batch_input_ids, batch_labels = batch_input_ids.to(device), batch_labels.to(device)  # Move batch to device\n",
    "            outputs = model(batch_input_ids, labels=batch_labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Average Loss = {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Save the final trained model state dictionary\n",
    "    final_model_state_dict_path = 'power_train.pth'\n",
    "    torch.save(model.state_dict(), final_model_state_dict_path)\n",
    "\n",
    "    return final_model_state_dict_path\n",
    "\n",
    "# Example usage:\n",
    "cuda_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trained_model_path = train_and_save_model(dataset_path, data_start_idx, data_end_idx, cuda_device)\n",
    "print(f\"Trained model saved at: {trained_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
